---
title: "Tidy Modeling Notes"
output: html_notebook
---

# Setup

```{r}
library(tidymodels)
library(tidyverse)
tidymodels_prefer()
```

# 5 Spending our Data Budget

## 5.1 Common Methods for Splitting Data

When data are reused for multiple tasks, instead of carefully 'spent' from the finite data budget, certain risks increase, such as the risk of accentuating bias or compounding effects from methodological errors

Allocate data to different tasks, not putting everything into parameter est.

Primary approach is normally **training** and **test** sets: 2 sets.

Training is the sandbox where exploration, feature engineering, and model fitting happens. Testing is where evaluate how well the training results work. **We have to use testing once and only once, or else it becomes part of the modeling process.**

*How should we conduct this split? Depends on the context.*

We could use random sampling with the `rsample` package. `initial_split()` was created for this. Let's try it out:

```{r}
library(tidymodels)
tidymodels_prefer()

# set the random number stream using `set.seed()` so that results can be reproduced later.
set.seed(501)

# save the split information for an 80/20 split, for example
ames_split <- initial_split(ames, prop = .8)
ames_split
```

The new object `ames_split` is an `rsplit` object and contains only the partitioning information; to get the resulting data sets, we apply two more functions:

```{r}
ames_train <- training(ames_split)
ames_test <- testing(ames_split)

dim(ames_train)
```

Simple sampling is useful in many cases but not all. Consider if we have a *class imbalance* and we need to avoid placing all of a popular class in our training set. We can use *stratified sampling*. We can also stratify quantitative values across quantiles.

Take our example. We have a skewed sale price variable. The worry here with simple splitting is that the more expensive houses would not be well represented in the training set; this would increase the risk that our model would be ineffective at predicting the price for such properties. A stratified random sample would conduct the 80/20 split within each of these data subsets and then pool the results. Let's try it using the `strata` argument:

```{r}
set.seed(502)
ames_split <- initial_split(ames, prop = .8, strata = log_sale_price)
ames_train <- training(ames_split)
ames_test <- testing(ames_split)
```

Only one column can be used for stratification. **There is very little downside to using stratified sampling.**

Are there any cases where random sampling is not the best choice? Yes. One is when the data have a significant time component, such as time series data. You might want to use the most recent test set. There's a variation of `initial_split` called `initial_time_split()` that, instead of using random sampling, the `prop` argument denotes what proportion of the first part of the data should be used as the training set.

There's complications in using time data. Too little data in the training set hampers the model's ability to find appropriate parameter estimates. Conversely, too little data in the test set lowers the quality of the performance estimates. Parts of the statistics community eschew test sets in general because they believe all of the data should be used for parameter estimation!! While there is merit to this argument, it is good modeling practice to have an unbiased set of observations as the final arbiter of model quality. A test set should be avoided only when the data are pathologically small.

## 5.2 What about a validation set?

# 6 Fitting Models with `parsnip`

## 6.1 Create a model

Other packages are heterogenous in how you provide the data to the functions. This makes memorization and syntax frustrating.

For tidymodels, the approach to specifying a model is intended to be more unified:

1.  Specify the type of model based on its mathematical structure (linear regression, random forest, KNN, etc)

2.  Specify the engine for fitting the model

3.  When required, declare the mode of the model (regression v classification; usually this is decided by the structure from step 1)

```{r}
linear_reg() %>% set_engine("lm")

linear_reg() %>% set_engine("glmnet")

linear_reg() %>% set_engine("stan")
```

The `translate()` function can provide details on how `parsnip` converts the user's code to the package's syntax:

```{r}
linear_reg() %>%
    set_engine("lm") %>% 
    translate()
```

Now let's walk through how to predict the sale price of houses in the Ames data as a function of only longitude and latitude:

```{r}
lm_model <- linear_reg() %>% 
    set_engine("lm")

lm_form_fit <- lm_model %>% 
    fit(
        log_sale_price ~ longitude + latitude,
        data = ames_train
    )

lm_xy_fit <- lm_model %>% 
    fit_xy(
        x = ames_train %>% select(longitude, latitude),
        y = ames_train %>% pull(log_sale_price)
    )

lm_form_fit

lm_xy_fit
```

## 6.2 Use the Model Results

```{r}
lm_form_fit %>% 
    extract_fit_engine()
```

```{r}
lm_form_fit %>% 
    extract_fit_engine() %>% vcov()
```

Base R summaries of models can be outdated. Let's use the `tidy` function instead:

```{r}
tidy(lm_form_fit)
```

## 6.3 Make Predictions

`parsnip` always uses the following rules:

1.  The results are always a tibble

2.  The column names of the tibble are always predictable

3.  There are always as many rows in the tibble as there are in the input data set

```{r}
ames_test_small <- ames_test %>% slice(1:5)

predict(lm_form_fit, new_data = ames_test_small)
```

These three rules make it easier to merge predictions with the original data:

```{r}
ames_test_small %>% 
    select(log_sale_price) %>% 
    bind_cols(
        predict(
            lm_form_fit, ames_test_small
        )
    ) %>% 
    # add 95% confidence intervals to the results:
    bind_cols(
        predict(
            lm_form_fit, ames_test_small, type = "pred_int"
        )
    )
```
